{
    // A cut-down version of the training confg used by kohya, only including the params we need to set.
    // NB NONE OF THIS IS BEING USED UNLESS WE CHANGE THE API ON THE KOHYA SIDE TO ACCEPT EVERY ONE OF THESE VALUES
    // The rest are provided on the gpu side.

    // This file is in JSON format, but must be converted to TOML format before being sent to the gpu side.

    // These must be synced with the training_session_manager.py run_training() function
    "id": "", // REMOVE BEFORE TRAINING - the id of the training session
    "webhook_url": "", // REMOVE BEFORE TRAINING - the webhook url to call when the training is complete
    "training_images_url": "", // REMOVE BEFORE TRAINING - the url of the s3 zip of training images and captions
    "checkpoint_url": "", // REMOVE BEFORE TRAINING - the url of the checkpoint from civitai
    "checkpoint_filename": "", // REMOVE BEFORE TRAINING - the filename of the checkpoint from civitai: important we set this explicitly and get the right value from civitail
    "civitai_key": "", // REMOVE BEFORE TRAINING - the civitai key
    "trigger_word": "", // REMOVE BEFORE TRAINING - the trigger word and class e.g "ohxw man"
    "upload_url": "", // REMOVE BEFORE TRAINING - the presignedurl of the s3 bucket to upload the checkpoint to
    // "pretrained_model_name_or_path": "", // DO NOT SEND THIS - this is set on the gpu side

    // Core params
    "output_name": "", // the name of the files it creates should be same as the name of the training or the first of the trigger word and class e.g "ohxw"
    "train_batch_size": 4, // might need to tweak this based on memory. 4 seems too high for 3090 24gb
    "network_module": "networks.lora", // the name of the thing we're creating
    "clip_skip": 1, // depends on the model but most models don't say, perhaps leave as 1 for now. 1 for sdxl, 2 for anime for some reason.
    "flip_aug": false,
    "fp8_base": false,
    "full_bf16": false,
    "full_fp16": false,
    "keep_tokens": 1, // identifies how many tokens from the start should NOT experience caption/tag shuffling. The most important tokens should always be kept at the start i.e trigger and class
    "resolution": "1024,1024",
    "max_train_epochs": 30, // this is the number of epochs to train for. 30 is a good default.

    // metadata
    "metadata_author": "", // this will be nice to provide - an opportunity for a vanity tag of our own?
    "metadata_description": "", // this will be nice to provide, should include name of the model used as this affects results so much
    "metadata_license": "", // ignore fo rnow
    "metadata_tags": "", // this will be nice to provide, perhaps per trigger word and class?
    "metadata_title": "", // this will be nice to provide, perhaps the trigger word

    // bucketting
    "bucket_reso_steps": 64,
    "min_bucket_reso": 256,
    "max_bucket_reso": 2048,

    // network, optimiser and lr scheduler
    "image_repeats": 4, // this is the number of times to repeat the images in the training set. It's thought to relate to train_batch_size, and to the iterations needed over the regularisation images, but it's not clear.
    "text_encoder_lr": 0.00003, // one of the key things to change, should be 1/10 of the unet_lr for characters
    "unet_lr": 0.0003, // one of the key things to change
    "network_alpha": 16, //should be half of the network_dim
    "network_dim": 32,
    "optimizer_type": "AdamW8bit", // was Adafactor might need to change
    "optimizer_args": "", //["scale_parameter=False", "relative_step=False", "warmup_init=False"],
    "lr_scheduler": "cosine_with_restarts",

    // sampling
    "sample_every_n_epochs": 1, // we'll want to try setting this up, but perhaps not through this crap sampler which doesn't even do adetailer
    "sample_every_n_steps": 0,
    "sample_prompts": "",
    "sample_sampler": "euler_a",

    // checkpointing
    "save_every_n_epochs": 1, // this is vital to getting multiple checkpoints output
}
