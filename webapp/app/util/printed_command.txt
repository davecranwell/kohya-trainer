Example of live TOML from kohya_ss

bucket_no_upscale = true
bucket_reso_steps = 64
cache_latents = true
cache_latents_to_disk = true
caption_extension = ".txt"
clip_skip = 1
dynamo_backend = "no"
enable_bucket = true
epoch = 15
gradient_accumulation_steps = 1
huber_c = 0.1
huber_schedule = "snr"
learning_rate = 0.0004
logging_dir = "/workspace/kohya_ss/logs"
loss_type = "l2"
lr_scheduler = "cosine"
lr_scheduler_args = []
lr_scheduler_num_cycles = 1
lr_scheduler_power = 1
max_bucket_reso = 2048
max_data_loader_n_workers = 0
max_grad_norm = 1
max_timestep = 1000
max_token_length = 75
max_train_epochs = 15
max_train_steps = 1600
min_bucket_reso = 256
mixed_precision = "fp16"
multires_noise_discount = 0.3
network_alpha = 32
network_args = []
network_dim = 64
network_module = "networks.lora"
no_half_vae = true
noise_offset_type = "Original"
optimizer_args = [ "scale_parameter=False", "relative_step=False", "warmup_init=False",]
optimizer_type = "Adafactor"
output_dir = "/workspace/kohya_ss/outputs"
output_name = "last"
pretrained_model_name_or_path = "/workspace/storage/stable_diffusion/models/ckpt/blah.safetensors"
prior_loss_weight = 1
resolution = "1024,1024"
sample_prompts = "/workspace/kohya_ss/outputs/prompt.txt"
sample_sampler = "euler_a"
save_every_n_epochs = 1
save_model_as = "safetensors"
save_precision = "bf16"
text_encoder_lr = 5e-5
train_batch_size = 4
train_data_dir = "/workspace/img"
unet_lr = 0.0001
xformers = true


python /workspace/kohya_ss/sdxl_train_network.py --session_id cm7s29baz0003slo4ki06kwvn --logging_dir runs/cm7s29baz0003slo4ki06kwvn/logs/training --output_dir runs/cm7s29baz0003slo4ki06kwvn/outputs/model --pretrained_model_name_or_path runs/pretrained/blah.safetensors --train_data_dir runs/cm7s29baz0003slo4ki06kwvn/train_data --network_module networks.lora --adaptive_noise_scale 0 --bucket_no_upscale --bucket_reso_steps 64 --cache_latents --cache_latents_to_disk --caption_dropout_every_n_epochs 0 --caption_dropout_rate 0 --caption_extension .txt --clip_skip 1 --enable_bucket --gradient_accumulation_steps 1 --gradient_checkpointing --huber_c 0.1 --huber_schedule snr --ip_noise_gamma 0 --keep_tokens 2 --learning_rate 0.0004 --loss_type l2 --lr_scheduler cosine --lr_scheduler_num_cycles 1 --lr_scheduler_power 1 --lr_warmup 5 --max_bucket_reso 2048 --max_data_loader_n_workers 0 --max_grad_norm 1 --resolution 1024,1024 --max_timestep 1000 --max_token_length 150 --max_train_epochs 15 --max_train_steps 0 --min_bucket_reso 256 --min_snr_gamma 0 --min_timestep 0 --mixed_precision bf16 --multires_noise_discount 0.3 --multires_noise_iterations 0 --network_alpha 16 --network_dim 32 --network_dropout 0 --noise_offset 0 --optimizer_type Adafactor --optimizer_args scale_parameter=False relative_step=False warmup_init=False --output_name ohxw --prior_loss_weight 1 --sample_every_n_epochs 0 --sample_every_n_steps 0 --sample_sampler euler_a --save_every_n_epochs 1 --save_every_n_steps 0 --save_last_n_steps 0 --save_last_n_steps_state 0 --save_model_as safetensors --save_precision bf16 --scale_weight_norms 0 --seed 0 --text_encoder_lr 4e-05 --train_batch_size 1 --unet_lr 0.0004 --v_pred_like_loss 0 --vae_batch_size 0 --xformers --metadata_author false  --metadata_description "Trigger word(s): ohxw"  --metadata_title 22365