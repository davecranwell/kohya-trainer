{
    // A cut-down version of the training confg used by kohya, only including the params we need to set.
    // NB NONE OF THIS IS BEING USED UNLESS WE CHANGE THE API ON THE KOHYA SIDE TO ACCEPT EVERY ONE OF THESE VALUES
    // The rest are provided on the gpu side.

    // This file is in JSON format, but must be converted to TOML format before being sent to the gpu side.

    // These must be synced with the training_session_manager.py run_training() function
    "id": "", // REMOVE BEFORE TRAINING - the id of the training session
    "webhook_url": "", // REMOVE BEFORE TRAINING - the webhook url to call when the training is complete
    "training_images_url": "", // REMOVE BEFORE TRAINING - the url of the s3 zip of training images and captions
    "checkpoint_url": "", // REMOVE BEFORE TRAINING - the url of the checkpoint from civitai
    "checkpoint_filename": "", // REMOVE BEFORE TRAINING - the filename of the checkpoint from civitai: important we set this explicitly and get the right value from civitail
    "civitai_key": "", // REMOVE BEFORE TRAINING - the civitai key
    "trigger_word": "", // REMOVE BEFORE TRAINING - the trigger word and class e.g "ohxw man"
    "subject_type": "", // REMOVE BEFORE TRAINING - the subject type e.g "woman", "object", "animal"
    "upload_url": "", // REMOVE BEFORE TRAINING - the presignedurl of the s3 bucket to upload the checkpoint to
    // "pretrained_model_name_or_path": "", // DO NOT SEND THIS - this is set on the gpu side

    // Core params
    "output_name": "", // the name of the files it creates should be same as the name of the training or the first of the trigger word and class e.g "ohxw"
    "train_batch_size": 4, // batch can improve generalisation of similar images, but averages dissimilar images in a way that loses detail overall. might need to tweak this based on memory. 4 seems too high for 3090 24gb
    "image_repeats": 4, // this is the number of times to repeat the images in the training set. It's thought to relate to train_batch_size, and to the iterations needed over the regularisation images, but it's not clear. It's value without regularisation is unclear too.
    "clip_skip": 1, // depends on the model but most models don't say, perhaps leave as 1 for now. 1 for sdxl, 2 for anime for some reason.

    "resolution": "1024,1024",
    "max_train_epochs": 150, // this is the number of epochs to train for. 30 is a good default. Max train epochs always overrides max_train_steps. Setting it to 0 means training NEVER STARTS!
    // "max_train_steps": 4000, // this is the number of steps to train for. 1500-3000 is usually enough

    // metadata
    "metadata_author": "", // this will be nice to provide - an opportunity for a vanity tag of our own?
    "metadata_description": "", // this will be nice to provide, should include name of the model used as this affects results so much
    "metadata_license": "", // ignore fo rnow
    "metadata_tags": "", // this will be nice to provide, perhaps per trigger word and class?
    "metadata_title": "", // this will be nice to provide, perhaps the trigger word

    // bucketting
    "bucket_reso_steps": 64,
    "min_bucket_reso": 256,
    "max_bucket_reso": 2048,

    // network, optimiser and lr scheduler
    "learning_rate": 0.0002, // this is the learning rate for the unet and it's believe that text_encoder_lr and unet_lr override it.
    "text_encoder_lr": 0.00002, // one of the key things to change, should be 1/10 of the unet_lr for characters.
    "unet_lr": 0.0002, // one of the key things to change, was 0.0003 (20250909).  With adafactor it seems this can be 0.0004 or more?, with prodigy it should be 1 because it's handled dynamically
    "network_alpha": 16, // Was 16.Some commentary here: https://rentry.org/59xed3#net-dim-network-dimensionsrank 
    "network_dim": 32, // Was 32. as above
    "network_args": [ "preset=full", "conv_dim=32", "conv_alpha=1", "use_tucker=False", "rank_dropout=0", "bypass_mode=False", "dora_wd=True", "module_dropout=0", "factor=-1", "use_cp=False", "use_scalar=False", "decompose_both=False", "rank_dropout_scale=False", "algo=locon", "train_norm=False"],
    "network_module": "lycoris.kohya", // the type of the network we're creating
    "optimizer_type": "AdamW8bit", //"AdamW8bit" or "Adafactor" or "Prodigy"
    "optimizer_args": "", // for Prodigy use: ["decouple=True", "weight_decay=0.01", "d_coef=0.8", "use_bias_correction=True", "safeguard_warmup=True", "betas=0.9,0.99"], this is needed for adafactor ["scale_parameter=False", "relative_step=False", "warmup_init=False"],
    "lr_scheduler": "cosine_with_restarts", // also: cosine_with_restarts
    "lr_warmup_steps": 150, // 10% of desired total steps (always 2000). Came from here: https://discuss.huggingface.co/t/perfect-lora-training-parameters-human-character/147211

    // sampling
    "sample_every_n_epochs": 1, // we'll want to try setting this up, but perhaps not through this crap sampler which doesn't even do adetailer
    "sample_every_n_steps": 0,
    "sample_prompts": "",
    "sample_sampler": "euler_a",

    // checkpointing
    "save_every_n_epochs": 1, // this is vital to getting multiple checkpoints output
    
    // other experimental stuff
    "cache_latents_to_disk": true,
    "min_snr_gamma": 5, // https://civitai.com/articles/3105/essential-to-advanced-guide-to-training-a-lora (also https://rentry.org/59xed3#min-snr-gamma_1)
    // "noise_offset_type": "Multires", // https://web.archive.org/web/20250810173210/https://civitai.com/articles/3105/essential-to-advanced-guide-to-training-a-lora
    // "multires_noise_iterations": 6, // https://web.archive.org/web/20250810173210/https://civitai.com/articles/3105/essential-to-advanced-guide-to-training-a-lora
    "shuffle_caption": true, // requires keep_tokens to be 1 assuming first token is trigger word.  
